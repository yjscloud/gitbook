# 8.1.2 实验架构设计

**本方案中是模拟环境，由六个OpenStack节点与NFS存储后端和YUM服务器组成。**

## 部署架构设计

* 第一部分是三个控制节点，控制节点上承载着NTP集群、Mariadb Galera集群、RabbitMQ集群、haproxy集群的主要工作。当某个服务宕机或服务中断之后，集群也将会自定切换，保证服务高可用性。另外控制节点上安装了对应的OpenStack组件，在云端起到了身份验证、镜像存储管理和部分计算节点控制，网络代理和后台管理界面等功能。
* 第二部分是计算节点，计算节点上承载着KVM虚拟化模块，起到计算资源虚拟化的作用，供给云端运行虚拟机实例。另外计算节点上安装了对应的OpenStack组件，在云端起到了接受和响应总段用户有关虚拟机的请求、网络代理连接虚拟网络等功能。可以通过横行扩展或是竖向扩展进行安装添加资源池，且新增计算节点配置部署方便快捷。
* 第三部分是后端存储。根据实际情况放弃了安装swift组件，在控制节点和计算节点上安装了镜像存储和块存储存储服务。后端存储使用NFS。NFS主要起到存储VM镜像、存储实例的功能；Cinder主要起到为实例挂载硬盘的作用。
* 第四部分是yum源的配置，OpenStack版本众多且版本的配置参数标准不统一，通过在阿里云上拷贝centos7.2、epel、OpenStack Newton版、Mariadb的rpm数据包。减低linux软件更新安装出现的各种不稳定因素。

![8-1-6](http://pded8ke3e.bkt.clouddn.com/8-1-6.png)

## 节点资源分析

根据方案设计的模型，需要安装8个centos mini 7.2系统，

| 节点 | 系统 | 数量 | CPU | 内存 | 硬盘\(GB\) | 网卡\(千兆\) | 支持虚拟化 |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Controller | Centos 7.2 | 3 | 4c | 8GB | 150 | 3 | 不支持 |  |
| Compute | Centos 7.2 | 2 | 4c+ | 8GB+ | 100 | 3 | 支持 |  |
| Cinder | Centos 7.2 | 1 | 4c | 4GB | 100+300 | 3 | 不支持 |  |
| Nfs |  | Centos 7.2 | 1 | 4c | 4GB | 100+ | 2 | 不支持 |
| Yum | Centos 7.2 | 1 | 4c | 512MB | 100 | 1 | 不支持 |  |

控制节点是私有云的控制节点，安装所有节点控制组件，负责私有云的调度、管理。每个节点CPU需要至少4核，内存至少8G，150GB硬盘。

计算节点是私有云的计算节点，负责创建，存储虚拟机，所有的实例都要支持虚拟化，并且CPU需要至少4核，内存8G+，100GB硬盘。

块存储节点是私有云的块设备存储节点，负责给虚拟机挂载使用，向虚拟机提供存储数据的磁盘空间。Cinder节点CPU需要4核，内存8G+，100GB+300GB硬盘。

NFS节点是私有云后端存储的节点，提供给glance组件存储镜像和nova 组件提供存储实例。所以该节点的CPU需要4核，内存4G，100GB硬盘。

YUM节点提供给其他节点安装软件，包含着centos7.2、epel、OpenStack Newton版、Mariadb的rpm数据包。所以该节点的CPU需要4核，内存512M，100GB硬盘。

各节点最低硬件需求： 

![8-1-7](http://pded8ke3e.bkt.clouddn.com/8-1-7.png)

## 网络需求

集群中控制节点和计算节点都需要连接3张网卡：exrernal网络，admin网络，tunnel网络。块存储节点不需要与外部网络联系，只需通过管理网络与核心节点交互数据即可。所以集群节点默认使用3张网卡，存储节点默认使用2张网卡。NFS和Yum只需要给内部计算节点和控制节点使用，只需内部网络。如果内网的电脑需要连接互联网，那么外部网络必须设置为flat模式，且该VLAN必须是外部网络，不然所有VM都不能访问互联网。

逻辑拓扑网络： 

![8-1-8](http://pded8ke3e.bkt.clouddn.com/8-1-8.png)

